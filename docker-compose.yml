# Ralph Wiggum harness (interactive, long-running containers).
# Example usage:
#   docker compose up -d
#   docker compose exec -T ralph ralph --help
#   docker compose exec -T slither slither . --json artifacts/slither.json
#   docker compose exec -T foundry forge test --fuzz-runs 256
services:
  ralph:
    build:
      context: .
      dockerfile: Dockerfile
    environment:
      BUDGET_TOKEN_LIMIT: "0"
      BUDGET_COST_LIMIT_USD: "0.0"
      BUDGET_TIME_LIMIT_SECONDS: "0"
      VLLM_BASE_URL: "${VLLM_BASE_URL:-}"
      VLLM_MODEL: "${VLLM_MODEL:-}"
      VLLM_API_KEY: "${VLLM_API_KEY:-}"
    volumes:
      - ./artifacts:/app/artifacts
      - .:/app
    working_dir: /app
    command: ["sh", "-lc", "sleep infinity"]

  slither:
    image: trailofbits/slither:latest
    volumes:
      - ./artifacts:/work/artifacts
      - .:/work
    working_dir: /work
    command: ["sh", "-lc", "sleep infinity"]

  foundry:
    image: ghcr.io/foundry-rs/foundry:latest
    volumes:
      - ./artifacts:/work/artifacts
      - .:/work
    working_dir: /work
    environment:
      FOUNDRY_FUZZ_RUNS: "256"
    command: ["sh", "-lc", "sleep infinity"]

  solodit:
    image: curlimages/curl:latest
    volumes:
      - ./artifacts:/work/artifacts
    working_dir: /work
    command: ["sh", "-c", "while true; do echo 'solodit stub'; sleep 3600; done"]

  vllm:
    image: vllm/vllm-openai:latest
    profiles: ["gpu"]
    environment:
      VLLM_MODEL: "${VLLM_MODEL:-}"
    volumes:
      - ./artifacts:/work/artifacts
    ports:
      - "8000:8000"
